{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8528de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d73bee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '../output/mlsurrogate/'\n",
    "# jobs = 20000\n",
    "job_sim_file = '/home/sairam/ATLASGRIDV2/ATLAS-GRID-SIMULATION/output/mlsurrogate/ml_surroagte_net2_JOBS.csv'\n",
    "state_sim_file = '/home/sairam/ATLASGRIDV2/ATLAS-GRID-SIMULATION/output/mlsurrogate/ml_surroagte_net2_STATE.csv'\n",
    "jobs_df = pd.read_csv(job_sim_file)\n",
    "states_df = pd.read_csv(state_sim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ecc1e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JOB_ID          SITE    CPU    STATUS  MEMORY  CORES         FLOPS  \\\n",
      "0  6088783954  NET2_Amherst  cpu-0  finished     0.0      1  8.731108e+10   \n",
      "1  6088783958  NET2_Amherst  cpu-0  finished     0.0      1  8.597622e+10   \n",
      "2  6088783961  NET2_Amherst  cpu-0  finished     0.0      1  8.637807e+10   \n",
      "3  6088783912  NET2_Amherst  cpu-0  finished     0.0      1  8.762557e+10   \n",
      "4  6088783944  NET2_Amherst  cpu-0  finished     0.0      1  8.996333e+10   \n",
      "\n",
      "   EXECUTION_TIME  IO_SIZE  IO_TIME  CPU_CONSUMPTION_TIME    CREATION_TIME  \\\n",
      "0       44774.912      0.0      0.0               49972.0  1/26/2024 17:34   \n",
      "1       44090.368      0.0      0.0               49208.0  1/26/2024 17:34   \n",
      "2       44296.448      0.0      0.0               49438.0  1/26/2024 17:34   \n",
      "3       44936.192      0.0      0.0               50152.0  1/26/2024 17:34   \n",
      "4       46135.040      0.0      0.0               51490.0  1/26/2024 17:34   \n",
      "\n",
      "            START_TIME             END_TIME  QUEUE_TIME  \n",
      "0  2024-01-22 12:47:00  2024-01-23 01:13:14         0.0  \n",
      "1  2024-01-22 12:47:00  2024-01-23 01:01:50         0.0  \n",
      "2  2024-01-22 12:47:00  2024-01-23 01:05:16         0.0  \n",
      "3  2024-01-22 12:47:00  2024-01-23 01:15:56         0.0  \n",
      "4  2024-01-22 12:47:00  2024-01-23 01:35:55         0.0  \n"
     ]
    }
   ],
   "source": [
    "print(jobs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ba2c9",
   "metadata": {},
   "source": [
    "Verifying the Jobs dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "052f82a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          JOB_ID          SITE     CPU    STATUS  MEMORY  CORES         FLOPS  \\\n",
      "0     6088783954  NET2_Amherst   cpu-0  finished     0.0      1  8.731108e+10   \n",
      "1     6088783958  NET2_Amherst   cpu-0  finished     0.0      1  8.597622e+10   \n",
      "2     6088783961  NET2_Amherst   cpu-0  finished     0.0      1  8.637807e+10   \n",
      "3     6088783912  NET2_Amherst   cpu-0  finished     0.0      1  8.762557e+10   \n",
      "4     6088783944  NET2_Amherst   cpu-0  finished     0.0      1  8.996333e+10   \n",
      "...          ...           ...     ...       ...     ...    ...           ...   \n",
      "2963  6084903914  NET2_Amherst  cpu-84  finished     0.0      1  5.416320e+07   \n",
      "2964  6084842925  NET2_Amherst  cpu-84  finished     0.0      1  1.207315e+09   \n",
      "2965  6084842926  NET2_Amherst  cpu-84  finished     0.0      1  1.174118e+09   \n",
      "2966  6084842931  NET2_Amherst  cpu-84  finished     0.0      1  1.179360e+09   \n",
      "2967  6084842932  NET2_Amherst  cpu-84  finished     0.0      1  1.389024e+09   \n",
      "\n",
      "      EXECUTION_TIME  IO_SIZE  IO_TIME  CPU_CONSUMPTION_TIME  \\\n",
      "0       44774.912000      0.0      0.0               49972.0   \n",
      "1       44090.368000      0.0      0.0               49208.0   \n",
      "2       44296.448000      0.0      0.0               49438.0   \n",
      "3       44936.192000      0.0      0.0               50152.0   \n",
      "4       46135.040000      0.0      0.0               51490.0   \n",
      "...              ...      ...      ...                   ...   \n",
      "2963       18.360407      0.0      0.0                  31.0   \n",
      "2964      409.259390      0.0      0.0                 691.0   \n",
      "2965      398.006237      0.0      0.0                 672.0   \n",
      "2966      399.783051      0.0      0.0                 675.0   \n",
      "2967      470.855593      0.0      0.0                 795.0   \n",
      "\n",
      "           CREATION_TIME          START_TIME            END_TIME  QUEUE_TIME  \\\n",
      "0    2024-01-26 17:34:00 2024-01-22 12:47:00 2024-01-23 01:13:14         0.0   \n",
      "1    2024-01-26 17:34:00 2024-01-22 12:47:00 2024-01-23 01:01:50         0.0   \n",
      "2    2024-01-26 17:34:00 2024-01-22 12:47:00 2024-01-23 01:05:16         0.0   \n",
      "3    2024-01-26 17:34:00 2024-01-22 12:47:00 2024-01-23 01:15:56         0.0   \n",
      "4    2024-01-26 17:34:00 2024-01-22 12:47:00 2024-01-23 01:35:55         0.0   \n",
      "...                  ...                 ...                 ...         ...   \n",
      "2963 2024-01-22 14:19:00 2024-01-22 12:47:00 2024-01-22 12:47:18         0.0   \n",
      "2964 2024-01-22 12:50:00 2024-01-22 12:47:00 2024-01-22 12:53:49         0.0   \n",
      "2965 2024-01-22 12:50:00 2024-01-22 12:47:00 2024-01-22 12:53:38         0.0   \n",
      "2966 2024-01-22 12:50:00 2024-01-22 12:47:00 2024-01-22 12:53:39         0.0   \n",
      "2967 2024-01-22 12:50:00 2024-01-22 12:47:00 2024-01-22 12:54:50         0.0   \n",
      "\n",
      "      valid_creation  valid_order  is_valid  \n",
      "0              False         True     False  \n",
      "1              False         True     False  \n",
      "2              False         True     False  \n",
      "3              False         True     False  \n",
      "4              False         True     False  \n",
      "...              ...          ...       ...  \n",
      "2963           False         True     False  \n",
      "2964           False         True     False  \n",
      "2965           False         True     False  \n",
      "2966           False         True     False  \n",
      "2967           False         True     False  \n",
      "\n",
      "[2815 rows x 18 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6926/1069307835.py:3: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  jobs_df[col] = pd.to_datetime(jobs_df[col], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_6926/1069307835.py:3: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  jobs_df[col] = pd.to_datetime(jobs_df[col], errors=\"coerce\", infer_datetime_format=True)\n",
      "/tmp/ipykernel_6926/1069307835.py:3: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  jobs_df[col] = pd.to_datetime(jobs_df[col], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "# Check if in the jobs dataframe, each job has a creation time less than or equal to end time \n",
    "for col in [\"CREATION_TIME\", \"START_TIME\", \"END_TIME\"]:\n",
    "    jobs_df[col] = pd.to_datetime(jobs_df[col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    \n",
    "jobs_df[\"valid_creation\"] = jobs_df[\"CREATION_TIME\"] < jobs_df[\"START_TIME\"]\n",
    "jobs_df[\"valid_order\"] = jobs_df[\"START_TIME\"] <= jobs_df[\"END_TIME\"]\n",
    "jobs_df[\"is_valid\"] = jobs_df[\"valid_creation\"] & jobs_df[\"valid_order\"]\n",
    "print(jobs_df[jobs_df[\"is_valid\"] == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "736ef3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID      JOB_ID CPU_NAME     STATE            TIMESTAMP          SITE  \\\n",
      "0   1  6088783954    cpu-0  assigned  2024-01-22 12:47:00  NET2_Amherst   \n",
      "1   2  6088783954    cpu-0   running  2024-01-22 12:47:00  NET2_Amherst   \n",
      "2   3  6088783958    cpu-0  assigned  2024-01-22 12:47:00  NET2_Amherst   \n",
      "3   4  6088783961    cpu-0  assigned  2024-01-22 12:47:00  NET2_Amherst   \n",
      "4   5  6088783912    cpu-0  assigned  2024-01-22 12:47:00  NET2_Amherst   \n",
      "\n",
      "   AVAILABLE_SITE_CORES  AVAILABLE_SITE_CPUS      WORKLOAD  NINPUT_FILES  \\\n",
      "0                  3488                  109  8.731108e+10             0   \n",
      "1                  3488                  109  8.731108e+10             0   \n",
      "2                  3487                  109  8.597622e+10             0   \n",
      "3                  3486                  109  8.637807e+10             0   \n",
      "4                  3485                  109  8.762557e+10             0   \n",
      "\n",
      "   NOUTPUT_FILES  INPUT_FILE_BYTES  OUTPUT_FILE_BYTES  SITE_PENDING_JOBS  \\\n",
      "0              3               0.0           744149.0                0.0   \n",
      "1              3               0.0           744149.0                0.0   \n",
      "2              3               0.0           672342.0                0.0   \n",
      "3              3               0.0           743505.0                0.0   \n",
      "4              3               0.0          2099608.0                0.0   \n",
      "\n",
      "   SITE_ASSIGNED_JOBS  SITE_FINISHED_JOBS  SITE_FAILED_JOBS  \n",
      "0                 1.0                 0.0               0.0  \n",
      "1                 1.0                 0.0               0.0  \n",
      "2                 2.0                 0.0               0.0  \n",
      "3                 3.0                 0.0               0.0  \n",
      "4                 4.0                 0.0               0.0  \n"
     ]
    }
   ],
   "source": [
    "print(states_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b81361",
   "metadata": {},
   "source": [
    "Verifying the States dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ad22ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6926/3549557007.py:2: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  states_df[\"TIMESTAMP\"] = pd.to_datetime(states_df[\"TIMESTAMP\"], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "# states_df = states_df.sort_values(by=\"TIMESTAMP\")\n",
    "states_df[\"TIMESTAMP\"] = pd.to_datetime(states_df[\"TIMESTAMP\"], errors=\"coerce\", infer_datetime_format=True)\n",
    "states_df[\"TIME_DELTA\"] = states_df[\"TIMESTAMP\"].diff().dt.total_seconds().fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "48b78293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [ID, JOB_ID, CPU_NAME, STATE, TIMESTAMP, SITE, AVAILABLE_SITE_CORES, AVAILABLE_SITE_CPUS, WORKLOAD, NINPUT_FILES, NOUTPUT_FILES, INPUT_FILE_BYTES, OUTPUT_FILE_BYTES, SITE_PENDING_JOBS, SITE_ASSIGNED_JOBS, SITE_FINISHED_JOBS, SITE_FAILED_JOBS, TIME_DELTA, valid]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "states_df[\"valid\"] = states_df[\"TIME_DELTA\"] >= 0\n",
    "print(states_df[states_df[\"valid\"] == False])\n",
    "states_df.to_csv(output_folder + 'z_processed_states.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc546851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model Summary======================\n",
      "Epoch 1/50, Train Loss: 9.8571, Test Job ID Loss: 8.4538, Test State Loss: 1.3870, Test Time Delta Loss: 0.0877\n",
      "Epoch 2/50, Train Loss: 10.1629, Test Job ID Loss: 8.4604, Test State Loss: 1.3880, Test Time Delta Loss: 0.0896\n",
      "Epoch 3/50, Train Loss: 9.6628, Test Job ID Loss: 8.4854, Test State Loss: 1.3861, Test Time Delta Loss: 0.0855\n",
      "Epoch 4/50, Train Loss: 9.9833, Test Job ID Loss: 8.5318, Test State Loss: 1.3864, Test Time Delta Loss: 0.0856\n",
      "Epoch 5/50, Train Loss: 9.3796, Test Job ID Loss: 8.5360, Test State Loss: 1.3895, Test Time Delta Loss: 0.0856\n",
      "Epoch 6/50, Train Loss: 9.6878, Test Job ID Loss: 8.5521, Test State Loss: 1.3866, Test Time Delta Loss: 0.0860\n",
      "Epoch 7/50, Train Loss: 9.3409, Test Job ID Loss: 8.5800, Test State Loss: 1.3883, Test Time Delta Loss: 0.0861\n",
      "Epoch 8/50, Train Loss: 9.4461, Test Job ID Loss: 8.5498, Test State Loss: 1.3872, Test Time Delta Loss: 0.0853\n",
      "Epoch 9/50, Train Loss: 9.5664, Test Job ID Loss: 8.5941, Test State Loss: 1.3875, Test Time Delta Loss: 0.0849\n",
      "Epoch 10/50, Train Loss: 9.7421, Test Job ID Loss: 8.6057, Test State Loss: 1.3865, Test Time Delta Loss: 0.0850\n",
      "Epoch 11/50, Train Loss: 9.5453, Test Job ID Loss: 8.6134, Test State Loss: 1.3884, Test Time Delta Loss: 0.0859\n",
      "Epoch 12/50, Train Loss: 9.2610, Test Job ID Loss: 8.6593, Test State Loss: 1.3917, Test Time Delta Loss: 0.0851\n",
      "Epoch 13/50, Train Loss: 9.7878, Test Job ID Loss: 8.6863, Test State Loss: 1.3878, Test Time Delta Loss: 0.0850\n",
      "Epoch 14/50, Train Loss: 9.4485, Test Job ID Loss: 8.7253, Test State Loss: 1.3913, Test Time Delta Loss: 0.0858\n",
      "Epoch 15/50, Train Loss: 9.8948, Test Job ID Loss: 8.8557, Test State Loss: 1.3882, Test Time Delta Loss: 0.0861\n",
      "Epoch 16/50, Train Loss: 9.1882, Test Job ID Loss: 9.1034, Test State Loss: 1.3929, Test Time Delta Loss: 0.0884\n",
      "Epoch 17/50, Train Loss: 9.3980, Test Job ID Loss: 9.2411, Test State Loss: 1.3940, Test Time Delta Loss: 0.0878\n",
      "Epoch 18/50, Train Loss: 8.7170, Test Job ID Loss: 9.5063, Test State Loss: 1.3905, Test Time Delta Loss: 0.0913\n",
      "Epoch 19/50, Train Loss: 8.0342, Test Job ID Loss: 9.8141, Test State Loss: 1.4023, Test Time Delta Loss: 0.0885\n",
      "Epoch 20/50, Train Loss: 8.2403, Test Job ID Loss: 10.1530, Test State Loss: 1.3991, Test Time Delta Loss: 0.0897\n",
      "Epoch 21/50, Train Loss: 7.6934, Test Job ID Loss: 10.5741, Test State Loss: 1.4077, Test Time Delta Loss: 0.0887\n",
      "Epoch 22/50, Train Loss: 7.8559, Test Job ID Loss: 10.9632, Test State Loss: 1.4066, Test Time Delta Loss: 0.0885\n",
      "Epoch 23/50, Train Loss: 7.2305, Test Job ID Loss: 11.5023, Test State Loss: 1.4180, Test Time Delta Loss: 0.0884\n",
      "Epoch 24/50, Train Loss: 6.9121, Test Job ID Loss: 11.9598, Test State Loss: 1.4104, Test Time Delta Loss: 0.0902\n",
      "Epoch 25/50, Train Loss: 6.1727, Test Job ID Loss: 12.4672, Test State Loss: 1.4150, Test Time Delta Loss: 0.0916\n",
      "Epoch 26/50, Train Loss: 5.3543, Test Job ID Loss: 13.1154, Test State Loss: 1.4176, Test Time Delta Loss: 0.0910\n",
      "Epoch 27/50, Train Loss: 5.7846, Test Job ID Loss: 13.6157, Test State Loss: 1.4180, Test Time Delta Loss: 0.0903\n",
      "Epoch 28/50, Train Loss: 4.2476, Test Job ID Loss: 14.2381, Test State Loss: 1.4215, Test Time Delta Loss: 0.0908\n",
      "Epoch 29/50, Train Loss: 5.0690, Test Job ID Loss: 14.6958, Test State Loss: 1.4178, Test Time Delta Loss: 0.0896\n",
      "Epoch 30/50, Train Loss: 3.9105, Test Job ID Loss: 15.4312, Test State Loss: 1.4198, Test Time Delta Loss: 0.0894\n",
      "Epoch 31/50, Train Loss: 5.0464, Test Job ID Loss: 15.9158, Test State Loss: 1.4379, Test Time Delta Loss: 0.0928\n",
      "Epoch 32/50, Train Loss: 4.5891, Test Job ID Loss: 16.5187, Test State Loss: 1.4278, Test Time Delta Loss: 0.0914\n",
      "Epoch 33/50, Train Loss: 4.6774, Test Job ID Loss: 17.1704, Test State Loss: 1.4196, Test Time Delta Loss: 0.0932\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "# --- 1. Define Model Parameters based on your data dimensions ---\n",
    "SEQUENCE_LENGTH = 10\n",
    "NUM_FEATURES = 17\n",
    "\n",
    "# These must match your actual label vocab sizes from preprocessing:\n",
    "# NUM_JOBS = np.unique(y_job_id_train_real).size\n",
    "# NUM_STATES = np.unique(y_state_train_real).size\n",
    "NUM_JOBS = 4000   # TODO: replace with actual unique JOB_ID count\n",
    "NUM_STATES = 4    # TODO: replace with actual unique STATE count\n",
    "\n",
    "# Hyperparameters for the Transformer\n",
    "D_MODEL = 64\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "FF_DIM = 128\n",
    "DROPOUT_RATE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Data split sizes (given)\n",
    "TOTAL_SEQUENCES = 12529\n",
    "TRAIN_SAMPLES = 10023\n",
    "TEST_SAMPLES = TOTAL_SEQUENCES - TRAIN_SAMPLES  # 2506\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Positional Encoding ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# --- 3. Multi-Task Transformer Model ---\n",
    "class GridSurrogate(nn.Module):\n",
    "    def __init__(self, num_features, d_model, num_heads, num_layers, ff_dim, num_jobs, num_states, dropout_rate=0.1):\n",
    "        super(GridSurrogate, self).__init__()\n",
    "        self.input_projection = nn.Linear(num_features, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=num_heads, dim_feedforward=ff_dim,\n",
    "            dropout=dropout_rate, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.job_id_head = nn.Linear(d_model, num_jobs)\n",
    "        self.state_head = nn.Linear(d_model, num_states)\n",
    "        self.time_delta_head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x_pooled = self.avg_pool(x.transpose(1, 2)).squeeze(2)\n",
    "        job_id_pred = self.job_id_head(x_pooled)\n",
    "        state_pred = self.state_head(x_pooled)\n",
    "        time_delta_pred = self.time_delta_head(x_pooled)\n",
    "        return job_id_pred, state_pred, time_delta_pred\n",
    "\n",
    "# --- 4. Instantiate and Train the Model ---\n",
    "\n",
    "# Dummy data generation with the CORRECT shapes; replace with your real data loading\n",
    "X_train = np.random.rand(TRAIN_SAMPLES, SEQUENCE_LENGTH, NUM_FEATURES).astype(np.float32)\n",
    "y_job_id_train = np.random.randint(0, NUM_JOBS, TRAIN_SAMPLES).astype(np.int64)\n",
    "y_state_train = np.random.randint(0, NUM_STATES, TRAIN_SAMPLES).astype(np.int64)\n",
    "y_time_delta_train = np.random.rand(TRAIN_SAMPLES).astype(np.float32)\n",
    "\n",
    "X_test = np.random.rand(TEST_SAMPLES, SEQUENCE_LENGTH, NUM_FEATURES).astype(np.float32)\n",
    "y_job_id_test = np.random.randint(0, NUM_JOBS, TEST_SAMPLES).astype(np.int64)\n",
    "y_state_test = np.random.randint(0, NUM_STATES, TEST_SAMPLES).astype(np.int64)\n",
    "y_time_delta_test = np.random.rand(TEST_SAMPLES).astype(np.float32)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float().to(device)\n",
    "y_job_id_train_tensor = torch.from_numpy(y_job_id_train).long().to(device)\n",
    "y_state_train_tensor = torch.from_numpy(y_state_train).long().to(device)\n",
    "y_time_delta_train_tensor = torch.from_numpy(y_time_delta_train).float().to(device)\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test).float().to(device)\n",
    "y_job_id_test_tensor = torch.from_numpy(y_job_id_test).long().to(device)\n",
    "y_state_test_tensor = torch.from_numpy(y_state_test).long().to(device)\n",
    "y_time_delta_test_tensor = torch.from_numpy(y_time_delta_test).float().to(device)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_job_id_train_tensor, y_state_train_tensor, y_time_delta_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_job_id_test_tensor, y_state_test_tensor, y_time_delta_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = GridSurrogate(NUM_FEATURES, D_MODEL, NUM_HEADS, NUM_LAYERS, FF_DIM, NUM_JOBS, NUM_STATES, DROPOUT_RATE).to(device)\n",
    "\n",
    "print(\"Model Summary======================\")\n",
    "# Define loss functions and optimizer\n",
    "criterion_job_id = nn.CrossEntropyLoss()\n",
    "criterion_state = nn.CrossEntropyLoss()\n",
    "criterion_time_delta = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch_X, batch_y_job_id, batch_y_state, batch_y_time_delta in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        job_id_pred, state_pred, time_delta_pred = model(batch_X)\n",
    "\n",
    "        # Calculate losses\n",
    "        loss_job_id = criterion_job_id(job_id_pred, batch_y_job_id)\n",
    "        loss_state = criterion_state(state_pred, batch_y_state)\n",
    "        loss_time_delta = criterion_time_delta(time_delta_pred.squeeze(), batch_y_time_delta)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = loss_job_id + loss_state + loss_time_delta\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        for batch_X, batch_y_job_id, batch_y_state, batch_y_time_delta in test_loader:\n",
    "            job_id_pred, state_pred, time_delta_pred = model(batch_X)\n",
    "            loss_job_id = criterion_job_id(job_id_pred, batch_y_job_id)\n",
    "            loss_state = criterion_state(state_pred, batch_y_state)\n",
    "            loss_time_delta = criterion_time_delta(time_delta_pred.squeeze(), batch_y_time_delta)\n",
    "            test_losses.append([loss_job_id.item(), loss_state.item(), loss_time_delta.item()])\n",
    "\n",
    "        avg_test_loss = np.mean(test_losses, axis=0)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{EPOCHS}, \"\n",
    "            f\"Train Loss: {total_loss.item():.4f}, \"\n",
    "            f\"Test Job ID Loss: {avg_test_loss[0]:.4f}, \"\n",
    "            f\"Test State Loss: {avg_test_loss[1]:.4f}, \"\n",
    "            f\"Test Time Delta Loss: {avg_test_loss[2]:.4f}\"\n",
    "        )\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'transformer_surrogate_model.pth')\n",
    "print(\"Model training complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "631dbdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64ce12e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data and targets saved successfully as CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Save the processed data and targets\n",
    "processed_features_df.to_csv('processed_features.csv', index=False)\n",
    "\n",
    "# Convert numpy arrays to pandas DataFrames and save as CSV\n",
    "y_job_id_df = pd.DataFrame(y_job_id, columns=['ENCODED_JOB_ID'])\n",
    "y_job_id_df.to_csv('y_job_id.csv', index=False)\n",
    "\n",
    "y_state_df = pd.DataFrame(y_state, columns=['ENCODED_STATE'])\n",
    "y_state_df.to_csv('y_state.csv', index=False)\n",
    "\n",
    "y_time_delta_df = pd.DataFrame(y_time_delta, columns=['TIME_DELTA'])\n",
    "y_time_delta_df.to_csv('y_time_delta.csv', index=False)\n",
    "\n",
    "print(\"Processed data and targets saved successfully as CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "afcd4c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m y_time_delta_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_time_delta_test)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Create TensorDatasets and DataLoaders\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_job_id_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_state_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_time_delta_train_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(X_test_tensor, y_job_id_test_tensor, y_state_test_tensor, y_time_delta_test_tensor)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:203\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    204\u001b[0m         tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[1;32m    205\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Define Model Parameters based on your data dimensions ---\n",
    "# Use the exact dimensions from your preprocessing step\n",
    "SEQUENCE_LENGTH = 10\n",
    "NUM_FEATURES = 127\n",
    "# These need to be derived from your preprocessed data (y_job_id, y_state)\n",
    "# Example: NUM_JOBS = np.unique(y_job_id).size\n",
    "# Example: NUM_STATES = np.unique(y_state).size\n",
    "# You need to run the preprocessing script and capture these values.\n",
    "NUM_JOBS = 4000  # Placeholder: Replace with the actual number of unique job IDs\n",
    "NUM_STATES = 4   # Placeholder: Replace with the actual number of unique states\n",
    "\n",
    "# Hyperparameters for the Transformer\n",
    "D_MODEL = 64\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "FF_DIM = 128\n",
    "DROPOUT_RATE = 0.1\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Define Positional Encoding Layer ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# --- 3. Define the Multi-Task Transformer Model ---\n",
    "class GridSurrogate(nn.Module):\n",
    "    def __init__(self, num_features, d_model, num_heads, num_layers, ff_dim, num_jobs, num_states, dropout_rate=0.1):\n",
    "        super(GridSurrogate, self).__init__()\n",
    "\n",
    "        # Initial projection of features to the model dimension\n",
    "        self.input_projection = nn.Linear(num_features, d_model)\n",
    "\n",
    "        # Positional encoding to add sequence order information\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # PyTorch's built-in transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Global average pooling to get a single vector representation\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Output heads for each task\n",
    "        self.job_id_head = nn.Linear(d_model, num_jobs)\n",
    "        self.state_head = nn.Linear(d_model, num_states)\n",
    "        self.time_delta_head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, num_features)\n",
    "\n",
    "        # Project input features\n",
    "        x = self.input_projection(x)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # Pass through the transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Average pooling to get a single context vector\n",
    "        x_pooled = self.avg_pool(x.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        # Make predictions with each head\n",
    "        job_id_pred = self.job_id_head(x_pooled)\n",
    "        state_pred = self.state_head(x_pooled)\n",
    "        time_delta_pred = self.time_delta_head(x_pooled)\n",
    "\n",
    "        return job_id_pred, state_pred, time_delta_pred\n",
    "\n",
    "# --- 4. Instantiate and Train the Model ---\n",
    "\n",
    "# Dummy data generation to make the code runnable\n",
    "# In a real scenario, you would load your actual preprocessed data here\n",
    "X_train = np.random.rand(10023, 10, 17).astype(np.float32)\n",
    "y_job_id_train = np.random.randint(0, NUM_JOBS, 8736).astype(np.int64)\n",
    "y_state_train = np.random.randint(0, NUM_STATES, 8736).astype(np.int64)\n",
    "y_time_delta_train = np.random.rand(8736).astype(np.float32)\n",
    "\n",
    "X_test = np.random.rand(2185, 10, 17).astype(np.float32)\n",
    "y_job_id_test = np.random.randint(0, NUM_JOBS, 2185).astype(np.int64)\n",
    "y_state_test = np.random.randint(0, NUM_STATES, 2185).astype(np.int64)\n",
    "y_time_delta_test = np.random.rand(2185).astype(np.float32)\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float().to(device)\n",
    "y_job_id_train_tensor = torch.from_numpy(y_job_id_train).long().to(device)\n",
    "y_state_train_tensor = torch.from_numpy(y_state_train).long().to(device)\n",
    "y_time_delta_train_tensor = torch.from_numpy(y_time_delta_train).float().to(device)\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test).float().to(device)\n",
    "y_job_id_test_tensor = torch.from_numpy(y_job_id_test).long().to(device)\n",
    "y_state_test_tensor = torch.from_numpy(y_state_test).long().to(device)\n",
    "y_time_delta_test_tensor = torch.from_numpy(y_time_delta_test).float().to(device)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_job_id_train_tensor, y_state_train_tensor, y_time_delta_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_job_id_test_tensor, y_state_test_tensor, y_time_delta_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = GridSurrogate(NUM_FEATURES, D_MODEL, NUM_HEADS, NUM_LAYERS, FF_DIM, NUM_JOBS, NUM_STATES, DROPOUT_RATE).to(device)\n",
    "\n",
    "# Define loss functions and optimizer\n",
    "criterion_job_id = nn.CrossEntropyLoss()\n",
    "criterion_state = nn.CrossEntropyLoss()\n",
    "criterion_time_delta = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch_X, batch_y_job_id, batch_y_state, batch_y_time_delta in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        job_id_pred, state_pred, time_delta_pred = model(batch_X)\n",
    "\n",
    "        # Calculate losses\n",
    "        loss_job_id = criterion_job_id(job_id_pred, batch_y_job_id)\n",
    "        loss_state = criterion_state(state_pred, batch_y_state)\n",
    "        loss_time_delta = criterion_time_delta(time_delta_pred.squeeze(), batch_y_time_delta)\n",
    "\n",
    "        # Combine losses (you can adjust weights here if needed)\n",
    "        total_loss = loss_job_id + loss_state + loss_time_delta\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        for batch_X, batch_y_job_id, batch_y_state, batch_y_time_delta in test_loader:\n",
    "            job_id_pred, state_pred, time_delta_pred = model(batch_X)\n",
    "\n",
    "            loss_job_id = criterion_job_id(job_id_pred, batch_y_job_id)\n",
    "            loss_state = criterion_state(state_pred, batch_y_state)\n",
    "            loss_time_delta = criterion_time_delta(time_delta_pred.squeeze(), batch_y_time_delta)\n",
    "\n",
    "            test_losses.append([loss_job_id.item(), loss_state.item(), loss_time_delta.item()])\n",
    "\n",
    "        avg_test_loss = np.mean(test_losses, axis=0)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, \"\n",
    "              f\"Train Loss: {total_loss.item():.4f}, \"\n",
    "              f\"Test Job ID Loss: {avg_test_loss[0]:.4f}, \"\n",
    "              f\"Test State Loss: {avg_test_loss[1]:.4f}, \"\n",
    "              f\"Test Time Delta Loss: {avg_test_loss[2]:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'transformer_surrogate_model.pth')\n",
    "print(\"Model training complete and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e2dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
